{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f510905-b501-4649-afa1-d8427269430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15748dc5-d65e-4209-af6b-5cf5a6e57d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c48ddf-55d1-4b54-a6a7-930dba5971d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UCSD Health Volunteer\\Code\\quantum-computing\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load IMDb dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = dataset['train'][:5000]  # Limiting the training size to 5000 samples\n",
    "test_dataset = dataset['test'][:500]  # Limiting the test size to 200 samples\n",
    "\n",
    "# Preprocess the text\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 4096  # Limiting the maximum length of tokens\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    return tokens['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f275d4-8122-49db-ab1c-486b2836cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum circuit\n",
    "n_qubits = 5\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def quantum_circuit(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# Quantum layer\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
    "        self.qlayer = qml.qnn.TorchLayer(quantum_circuit, self.weight_shapes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.qlayer(x)\n",
    "\n",
    "# Sentiment analysis model\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        super(SentimentAnalysisModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(tokenizer.vocab_size, n_qubits)\n",
    "        self.quantum_layer = QuantumLayer(n_qubits, n_layers)\n",
    "        self.fc = nn.Linear(n_qubits, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.quantum_layer(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c507f101-4956-463e-8c47-9b773186c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model = SentimentAnalysisModel(n_qubits, n_layers=2).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.2)\n",
    "\n",
    "# Prepare training data\n",
    "train_inputs = torch.stack([preprocess(text) for text in train_dataset['text']]).to(device)\n",
    "train_labels = torch.tensor(train_dataset['label'], dtype=torch.float32).unsqueeze(1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23163aa-23d7-422b-900d-1cced1f030a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.698544442653656, Train Accuracy: 50.12%\n",
      "Epoch 2/100, Loss: 0.7190716862678528, Train Accuracy: 49.88%\n",
      "Epoch 3/100, Loss: 0.6933144927024841, Train Accuracy: 49.88%\n",
      "Epoch 4/100, Loss: 0.6950789093971252, Train Accuracy: 50.12%\n",
      "Epoch 5/100, Loss: 0.6962664723396301, Train Accuracy: 50.12%\n",
      "Epoch 6/100, Loss: 0.6947662830352783, Train Accuracy: 50.12%\n",
      "Epoch 7/100, Loss: 0.6929341554641724, Train Accuracy: 50.12%\n",
      "Epoch 8/100, Loss: 0.6935946345329285, Train Accuracy: 49.88%\n",
      "Epoch 9/100, Loss: 0.6932476162910461, Train Accuracy: 49.88%\n",
      "Epoch 10/100, Loss: 0.6938587427139282, Train Accuracy: 50.12%\n",
      "Epoch 11/100, Loss: 0.6933603286743164, Train Accuracy: 50.12%\n",
      "Epoch 12/100, Loss: 0.6926799416542053, Train Accuracy: 49.88%\n",
      "Epoch 13/100, Loss: 0.6925914287567139, Train Accuracy: 49.88%\n",
      "Epoch 14/100, Loss: 0.6914692521095276, Train Accuracy: 49.88%\n",
      "Epoch 15/100, Loss: 0.6907579302787781, Train Accuracy: 52.42%\n",
      "Epoch 16/100, Loss: 0.6892753839492798, Train Accuracy: 54.36%\n",
      "Epoch 17/100, Loss: 0.6879702806472778, Train Accuracy: 50.04%\n",
      "Epoch 18/100, Loss: 0.6857511401176453, Train Accuracy: 62.42%\n",
      "Epoch 19/100, Loss: 0.6841871738433838, Train Accuracy: 53.00%\n",
      "Epoch 20/100, Loss: 0.6815750002861023, Train Accuracy: 51.96%\n",
      "Epoch 21/100, Loss: 0.6777045130729675, Train Accuracy: 68.02%\n",
      "Epoch 22/100, Loss: 0.6739463806152344, Train Accuracy: 77.50%\n",
      "Epoch 23/100, Loss: 0.669284462928772, Train Accuracy: 61.22%\n",
      "Epoch 24/100, Loss: 0.671928882598877, Train Accuracy: 50.96%\n",
      "Epoch 25/100, Loss: 0.668563961982727, Train Accuracy: 52.40%\n",
      "Epoch 26/100, Loss: 0.6501474976539612, Train Accuracy: 82.50%\n",
      "Epoch 27/100, Loss: 0.6569759845733643, Train Accuracy: 52.40%\n",
      "Epoch 28/100, Loss: 0.6413809657096863, Train Accuracy: 60.58%\n",
      "Epoch 29/100, Loss: 0.6310024261474609, Train Accuracy: 65.64%\n",
      "Epoch 30/100, Loss: 0.6299806237220764, Train Accuracy: 58.04%\n",
      "Epoch 31/100, Loss: 0.6083383560180664, Train Accuracy: 83.36%\n",
      "Epoch 32/100, Loss: 0.6092854142189026, Train Accuracy: 63.50%\n",
      "Epoch 33/100, Loss: 0.5943048596382141, Train Accuracy: 67.02%\n",
      "Epoch 34/100, Loss: 0.5700168609619141, Train Accuracy: 84.48%\n",
      "Epoch 35/100, Loss: 0.5649622678756714, Train Accuracy: 71.86%\n",
      "Epoch 36/100, Loss: 0.595364511013031, Train Accuracy: 58.64%\n",
      "Epoch 37/100, Loss: 0.6067555546760559, Train Accuracy: 58.98%\n",
      "Epoch 38/100, Loss: 0.5190315842628479, Train Accuracy: 84.52%\n",
      "Epoch 39/100, Loss: 0.5893282294273376, Train Accuracy: 58.92%\n",
      "Epoch 40/100, Loss: 0.506801187992096, Train Accuracy: 80.42%\n",
      "Epoch 41/100, Loss: 0.546720564365387, Train Accuracy: 65.98%\n",
      "Epoch 42/100, Loss: 0.48646679520606995, Train Accuracy: 84.60%\n",
      "Epoch 43/100, Loss: 0.51454758644104, Train Accuracy: 70.02%\n",
      "Epoch 44/100, Loss: 0.47319936752319336, Train Accuracy: 81.12%\n",
      "Epoch 45/100, Loss: 0.4770524799823761, Train Accuracy: 76.76%\n",
      "Epoch 46/100, Loss: 0.47098803520202637, Train Accuracy: 75.98%\n",
      "Epoch 47/100, Loss: 0.4286253750324249, Train Accuracy: 87.96%\n",
      "Epoch 48/100, Loss: 0.4485929012298584, Train Accuracy: 78.22%\n",
      "Epoch 49/100, Loss: 0.4120710492134094, Train Accuracy: 85.10%\n",
      "Epoch 50/100, Loss: 0.401824027299881, Train Accuracy: 85.46%\n",
      "Epoch 51/100, Loss: 0.42164310812950134, Train Accuracy: 80.08%\n",
      "Epoch 52/100, Loss: 0.38709133863449097, Train Accuracy: 84.24%\n",
      "Epoch 53/100, Loss: 0.3609957695007324, Train Accuracy: 90.10%\n",
      "Epoch 54/100, Loss: 0.37344223260879517, Train Accuracy: 85.82%\n",
      "Epoch 55/100, Loss: 0.3753100633621216, Train Accuracy: 82.58%\n",
      "Epoch 56/100, Loss: 0.3472183346748352, Train Accuracy: 87.88%\n",
      "Epoch 57/100, Loss: 0.31967347860336304, Train Accuracy: 92.08%\n",
      "Epoch 58/100, Loss: 0.3257767856121063, Train Accuracy: 88.90%\n",
      "Epoch 59/100, Loss: 0.328961044549942, Train Accuracy: 87.06%\n",
      "Epoch 60/100, Loss: 0.30083587765693665, Train Accuracy: 91.28%\n",
      "Epoch 61/100, Loss: 0.2820631265640259, Train Accuracy: 93.40%\n",
      "Epoch 62/100, Loss: 0.2875208258628845, Train Accuracy: 90.88%\n",
      "Epoch 63/100, Loss: 0.2963692247867584, Train Accuracy: 88.02%\n",
      "Epoch 64/100, Loss: 0.29367175698280334, Train Accuracy: 87.50%\n",
      "Epoch 65/100, Loss: 0.2665965259075165, Train Accuracy: 91.06%\n",
      "Epoch 66/100, Loss: 0.2387728989124298, Train Accuracy: 94.22%\n",
      "Epoch 67/100, Loss: 0.22895380854606628, Train Accuracy: 94.60%\n",
      "Epoch 68/100, Loss: 0.23639126121997833, Train Accuracy: 92.74%\n",
      "Epoch 69/100, Loss: 0.2543056011199951, Train Accuracy: 89.50%\n",
      "Epoch 70/100, Loss: 0.26565927267074585, Train Accuracy: 87.60%\n",
      "Epoch 71/100, Loss: 0.24291200935840607, Train Accuracy: 89.66%\n",
      "Epoch 72/100, Loss: 0.19457176327705383, Train Accuracy: 95.88%\n",
      "Epoch 73/100, Loss: 0.19381967186927795, Train Accuracy: 95.34%\n",
      "Epoch 74/100, Loss: 0.21423007547855377, Train Accuracy: 91.80%\n",
      "Epoch 75/100, Loss: 0.18857568502426147, Train Accuracy: 95.02%\n",
      "Epoch 76/100, Loss: 0.16344498097896576, Train Accuracy: 97.20%\n",
      "Epoch 77/100, Loss: 0.17803409695625305, Train Accuracy: 94.68%\n",
      "Epoch 78/100, Loss: 0.17779961228370667, Train Accuracy: 94.24%\n",
      "Epoch 79/100, Loss: 0.15002672374248505, Train Accuracy: 96.90%\n",
      "Epoch 80/100, Loss: 0.14141348004341125, Train Accuracy: 97.26%\n",
      "Epoch 81/100, Loss: 0.15383046865463257, Train Accuracy: 95.86%\n",
      "Epoch 82/100, Loss: 0.15026214718818665, Train Accuracy: 95.76%\n",
      "Epoch 83/100, Loss: 0.12821732461452484, Train Accuracy: 97.62%\n",
      "Epoch 84/100, Loss: 0.1200653538107872, Train Accuracy: 98.12%\n",
      "Epoch 85/100, Loss: 0.12801942229270935, Train Accuracy: 97.00%\n",
      "Epoch 86/100, Loss: 0.1239740177989006, Train Accuracy: 97.44%\n",
      "Epoch 87/100, Loss: 0.10822177678346634, Train Accuracy: 97.96%\n",
      "Epoch 88/100, Loss: 0.10524365305900574, Train Accuracy: 97.94%\n",
      "Epoch 89/100, Loss: 0.10977936536073685, Train Accuracy: 97.94%\n",
      "Epoch 90/100, Loss: 0.10244140028953552, Train Accuracy: 98.10%\n",
      "Epoch 91/100, Loss: 0.09269001334905624, Train Accuracy: 98.68%\n",
      "Epoch 92/100, Loss: 0.09388469904661179, Train Accuracy: 98.60%\n",
      "Epoch 93/100, Loss: 0.09453105181455612, Train Accuracy: 98.20%\n",
      "Epoch 94/100, Loss: 0.08694415539503098, Train Accuracy: 98.78%\n",
      "Epoch 95/100, Loss: 0.08149714022874832, Train Accuracy: 98.86%\n",
      "Epoch 96/100, Loss: 0.0828939899802208, Train Accuracy: 98.66%\n",
      "Epoch 97/100, Loss: 0.08187338709831238, Train Accuracy: 98.82%\n",
      "Epoch 98/100, Loss: 0.07571694999933243, Train Accuracy: 98.96%\n",
      "Epoch 99/100, Loss: 0.07221857458353043, Train Accuracy: 99.08%\n",
      "Epoch 100/100, Loss: 0.07290645688772202, Train Accuracy: 99.06%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_inputs)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Calculate training accuracy\n",
    "    predictions = (outputs >= 0.5).float()\n",
    "    correct_predictions = (predictions == train_labels).sum().item()\n",
    "    train_accuracy = correct_predictions / len(train_labels)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Train Accuracy: {train_accuracy * 100:.2f}%')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f3352b-62de-4b31-8baa-2693777936b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.40634748339653015\n",
      "Test Accuracy: 83.80%\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "test_inputs = torch.stack([preprocess(text) for text in test_dataset['text']]).to(device)\n",
    "test_labels = torch.tensor(test_dataset['label'], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_inputs)\n",
    "    test_loss = criterion(test_outputs, test_labels)\n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predictions = (test_outputs >= 0.5).float()\n",
    "    correct_predictions = (predictions == test_labels).sum().item()\n",
    "    accuracy = correct_predictions / len(test_labels)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c742b1d3-ad4c-4f9c-9d39-ab197a3c896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"This movie was fantastic! I loved it.\"\n",
      "Sentiment: positive (Score: 0.6165)\n",
      "\n",
      "Text: \"The film was terrible and a waste of time.\"\n",
      "Sentiment: negative (Score: 0.4318)\n",
      "\n",
      "Text: \"An average movie with some good moments.\"\n",
      "Sentiment: positive (Score: 0.5120)\n",
      "\n",
      "Text: \"Absolutely brilliant! A must-watch.\"\n",
      "Sentiment: positive (Score: 0.5694)\n",
      "\n",
      "Text: \"Not my cup of tea. I didn't enjoy it.\"\n",
      "Sentiment: positive (Score: 0.5484)\n",
      "\n",
      "Text: \"Absolutely worst!\"\n",
      "Sentiment: negative (Score: 0.4772)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to predict sentiment\n",
    "def predict_sentiment(text, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = preprocess(text).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        output = model(input_ids)\n",
    "        prediction = output.item()\n",
    "        sentiment = 'positive' if prediction >= 0.5 else 'negative'\n",
    "        return sentiment, prediction\n",
    "\n",
    "# Test the model on new data\n",
    "test_texts = [\n",
    "    \"This movie was fantastic! I loved it.\",\n",
    "    \"The film was terrible and a waste of time.\",\n",
    "    \"An average movie with some good moments.\",\n",
    "    \"Absolutely brilliant! A must-watch.\",\n",
    "    \"Not my cup of tea. I didn't enjoy it.\",\n",
    "    \"Absolutely worst!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment, score = predict_sentiment(text, model)\n",
    "    print(f'Text: \"{text}\"\\nSentiment: {sentiment} (Score: {score:.4f})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0cf6e-9e8c-4503-9b3c-71eef02d52db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
